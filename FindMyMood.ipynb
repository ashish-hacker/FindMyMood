{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re , string , random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Data/text_emotion.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import twitter_samples, stopwords\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import FreqDist, classify, NaiveBayesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_noise(tweet_tokens, stop_words = ()):\n",
    "\n",
    "    cleaned_tokens = []\n",
    "\n",
    "    for token, tag in pos_tag(tweet_tokens):\n",
    "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
    "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
    "\n",
    "        if tag.startswith(\"NN\"):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        token = lemmatizer.lemmatize(token, pos)\n",
    "\n",
    "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
    "            cleaned_tokens.append(token.lower())\n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_words(cleaned_tokens_list):\n",
    "    for tokens in cleaned_tokens_list:\n",
    "        for token in tokens:\n",
    "            yield token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_for_model(cleaned_tokens_list):\n",
    "    for tweet_tokens in cleaned_tokens_list:\n",
    "        yield dict([token, True] for token in tweet_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral_array = [i.split(' ') for i in np.array(data.content[data.sentiment == 'neutral'])]\n",
    "worry_array =[i.split(' ') for i in  np.array(data.content[data.sentiment == 'worry'])]\n",
    "happy_array = [ i.split(' ') for i in np.array(data.content[data.sentiment == 'happiness'])]\n",
    "sad_array = [ i.split(' ') for i in np.array(data.content[data.sentiment == 'sadness'])]\n",
    "love_array = [ i.split(' ') for i in np.array(data.content[data.sentiment == 'love'])]\n",
    "surprise_array = [ i.split(' ') for i in np.array(data.content[data.sentiment == 'surprise'])]\n",
    "fun_array = [ i.split(' ') for i in np.array(data.content[data.sentiment == 'fun'])]\n",
    "relief_array = [ i.split(' ') for i in np.array(data.content[data.sentiment == 'relief'])]\n",
    "hate_array = [ i.split(' ') for i in np.array(data.content[data.sentiment == 'hate'])]\n",
    "empty_array = [i.split(' ') for i in np.array(data.content[data.sentiment == 'empty'])]\n",
    "enthu_array = [ i.split(' ') for i in np.array(data.content[data.sentiment == 'enthusiasm'])]\n",
    "bore_array = [ i.split(' ') for i in np.array(data.content[data.sentiment == 'boredom'])]\n",
    "anger_array =[ i.split(' ') for i in np.array(data.content[data.sentiment == 'anger'])]\n",
    "\n",
    "n_arrays = [neutral_array, worry_array, happy_array, sad_array, love_array, surprise_array, fun_array, relief_array, hate_array, empty_array, enthu_array, anger_array, bore_array]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral_dat  = []\n",
    "worry_dat  = []\n",
    "happ_dat = []\n",
    "sad_dat = []\n",
    "love_dat = []   \n",
    "surprise_dat = [] \n",
    "fun_dat = [] \n",
    "relief_dat = [] \n",
    "hate_dat = []\n",
    "empty_dat = []\n",
    "enth_dat = []\n",
    "bore_dat= []\n",
    "anger_dat = []\n",
    "\n",
    "arrays = [neutral_dat, worry_dat, happ_dat, sad_dat, love_dat, surprise_dat, fun_dat, relief_dat, hate_dat, empty_dat, enth_dat, anger_dat, bore_dat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, k in zip(n_arrays, arrays):\n",
    "    for j in i:\n",
    "        j = [x for x in j if x!='']\n",
    "        k.append(remove_noise(j,stop_words))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#happy_array[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#happ_dat[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pos_words = get_all_words(neutral_dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('get', 705), ('go', 660), (\"i'm\", 468), ('good', 385), ('work', 351), ('like', 339), ('day', 331), ('one', 285), ('think', 264), ('back', 263)]\n"
     ]
    }
   ],
   "source": [
    "freq_dist_pos = FreqDist(all_pos_words)\n",
    "print(freq_dist_pos.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "happy_model = get_words_for_model(happ_dat)\n",
    "neutral_model = get_words_for_model(neutral_dat)\n",
    "worry_model = get_words_for_model(worry_dat)\n",
    "sad_model = get_words_for_model(sad_dat)\n",
    "love_model = get_words_for_model(love_dat)\n",
    "surprise_model = get_words_for_model(surprise_dat)\n",
    "fun_model = get_words_for_model(fun_dat)\n",
    "relief_model = get_words_for_model(relief_dat)\n",
    "hate_model = get_words_for_model(hate_dat)\n",
    "empty_model = get_words_for_model(empty_dat)\n",
    "enth_model = get_words_for_model(enth_dat)\n",
    "bore_model = get_words_for_model(bore_dat)\n",
    "anger_model = get_words_for_model(anger_dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "happy_dataset = [(tweet_dict, \"happy\")\n",
    "                         for tweet_dict in happy_model]\n",
    "neutral_dataset = [(tweet_dict, \"neutral\")\n",
    "                         for tweet_dict in neutral_model]\n",
    "worry_dataset = [(tweet_dict, \"worry\")\n",
    "                         for tweet_dict in worry_model]\n",
    "sad_dataset = [(tweet_dict, \"sad\")\n",
    "                         for tweet_dict in sad_model]\n",
    "love_dataset = [(tweet_dict, \"love\")\n",
    "                         for tweet_dict in love_model]\n",
    "hate_dataset = [(tweet_dict, \"hate\")\n",
    "                         for tweet_dict in hate_model]\n",
    "surprise_dataset = [(tweet_dict, \"surprise\")\n",
    "                         for tweet_dict in surprise_model]\n",
    "fun_dataset = [(tweet_dict, \"fun\")\n",
    "                         for tweet_dict in fun_model]\n",
    "relief_dataset = [(tweet_dict, \"relief\")\n",
    "                         for tweet_dict in relief_model]\n",
    "empty_dataset = [(tweet_dict, \"empty\")\n",
    "                         for tweet_dict in empty_model]\n",
    "enth_dataset = [(tweet_dict, \"enthusiastic\")\n",
    "                         for tweet_dict in enth_model]\n",
    "bore_dataset = [(tweet_dict, \"boredom\")\n",
    "                         for tweet_dict in bore_model]\n",
    "anger_dataset = [(tweet_dict, \"angry\")\n",
    "                         for tweet_dict in anger_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = anger_dataset + bore_dataset + enth_dataset + empty_dataset + relief_dataset + fun_dataset + surprise_dataset + hate_dataset + love_dataset + sad_dataset + worry_dataset + neutral_dataset + happy_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = dataset[:35000]\n",
    "test_data = dataset[35000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = NaiveBayesClassifier.train(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.1318\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy is:\", classify.accuracy(classifier, test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                    hate = True             hate : happy  =    132.0 : 1.0\n",
      "                    bore = True           boredo : happy  =    127.7 : 1.0\n",
      "                   stuck = True           boredo : neutra =     86.0 : 1.0\n",
      "                  freeze = True            angry : neutra =     85.8 : 1.0\n",
      "                  empire = True            angry : neutra =     85.8 : 1.0\n",
      "                    age. = True            angry : neutra =     85.8 : 1.0\n",
      "                 goooood = True            angry : neutra =     85.8 : 1.0\n",
      "                   drove = True            angry : neutra =     85.8 : 1.0\n",
      "                bullshit = True            angry : neutra =     85.8 : 1.0\n",
      "                    wing = True            angry : neutra =     85.8 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(classifier.show_most_informative_features(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_sentence = \"Let's party!\"\n",
    "custom_tokens = remove_noise(word_tokenize(custom_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: Let's party! \n",
      "Emotion: fun\n"
     ]
    }
   ],
   "source": [
    "print('Sentence:',custom_sentence,'\\nEmotion:', classifier.classify(dict([token, True] for token in custom_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from nltk.classify.scikitlearn import SklearnClassifier\n",
    "#from sklearn.naive_bayes import MultinomialNB,BernoulliNB\n",
    "#MNB_classifier = SklearnClassifier(MultinomialNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MNB_classifier.train(train_data)\n",
    "#print(\"MultinomialNB accuracy percent:\",classify.accuracy(MNB_classifier, test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BNB_classifier = SklearnClassifier(BernoulliNB())\n",
    "#BNB_classifier.train(train_data)\n",
    "#print(\"BernoulliNB accuracy percent:\",classify.accuracy(BNB_classifier, test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
    "#from sklearn.svm import SVC, LinearSVC, NuSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Original Naive Bayes Algo accuracy percent:\", (classify.accuracy(classifier, test_data))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LogisticRegression_classifier = SklearnClassifier(LogisticRegression(max_iter=500))\n",
    "#LogisticRegression_classifier.train(train_data)\n",
    "#print(\"LogisticRegression_classifier accuracy percent:\", (classify.accuracy(LogisticRegression_classifier, test_data))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SGDClassifier_classifier = SklearnClassifier(SGDClassifier())\n",
    "#SGDClassifier_classifier.train(train_data)\n",
    "#print(\"SGDClassifier_classifier accuracy percent:\", (classify.accuracy(SGDClassifier_classifier, test_data))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVC_classifier = SklearnClassifier(SVC())\n",
    "#SVC_classifier.train(train_data)\n",
    "#print(\"SVC_classifier accuracy percent:\", (classify.accuracy(SVC_classifier, test_data))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LinearSVC_classifier = SklearnClassifier(LinearSVC())\n",
    "#LinearSVC_classifier.train(train_data)\n",
    "#print(\"LinearSVC_classifier accuracy percent:\", (classify.accuracy(LinearSVC_classifier, test_data))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('Sentence:',custom_sentence,'\\nEmotion:', SVC_classifier.classify(dict([token, True] for token in custom_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
